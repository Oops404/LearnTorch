# -*- coding: UTF-8-*-
"""
@Project: LearnTorch
@Author: Oops404
@Email: cheneyjin@outlook.com
@Time: 2022/1/22 19:30
"""
"""
求导路径
    w -Σ-> z -sigmoid()-> sigma -BCEloss()-> loss 
         <----------反过来求导，即得w--------

    ∂Loss / ∂w，其中
    Loss = -Σ(i = 1~m)(y_i * ln(σ_i) + (1 - y_i) * ln(1 - σ_i))
这是单层，如果是双层、n层呢？多层传递后公式多层嵌套再偏导，极其复杂/(ㄒoㄒ)/。
    
反向传播算法DP：
链式求导救我！(ง •_•)ง
假设有函数 u=h(z), z=f(w), 且两函数在各自自变量的定义域上都可导，则有：
    ∂u/∂w = ∂u/∂z * ∂z/w
这里用↑表示神经网络表达式中的层号， ↑层↓序。
若网络有两层：
    ∂Loss / ∂w↑(1→2) = ∂L(σ) / ∂σ * ∂σ(z) / ∂z * ∂z(w) / ∂w
    
    ∂L(σ) / ∂σ = ∂(-Σ(i=1~m) (y_i * ln(σ_i) + (1 - y_i) * ln(1 - σ_i))) / ∂σ
               = Σ(i=1~m) (∂(-(y_i * ln(σ_i) + (1 - y_i) * ln(1 - σ_i))) / ∂σ)
前进提要，求导不影响加和，因此先加和还是先求导无所谓，先不看它。
    =-(y * 1 / σ + (1 - y) * 1 / (1 - σ) * (-1))
    =-(y / σ + (y - 1) / (1 - σ))
    =-(y(1 - σ) + (y - 1)σ) / (σ(1 - σ))
    =-(y - yσ + yσ - σ) / σ(1 - σ)
    =(σ - y) / σ(1 - σ)
    
    ∂σ(z) / ∂z = ∂(sigmoid(z)) / ∂z
               = ∂(1 / (1 + e^(-z))) / ∂z
               = ∂((1 + e^(-z))^(-1)) / ∂z
               = -1 * (1 + e^(-z))^(-2) * e^(-z) * (-1)
               = e^(-z) / (1 + e^(-z))^2
             
               = (1 + e^(-z) - 1) / (1 + e^(-z))^2
               = ((1 + e^(-z)) / (1 + e^(-z))^2) - (1 / (1 + e^(-z))^2)
               = (1 / (1 + e^(-z))) - (1 / (1 + e^(-z))^2)
               = (1 / (1 + e^(-z))) * (1 - (1 / (1 + e^(-z))))
               = σ(1 - σ)
此时的σ还是第二层的σ，因此接着：
    ∂z(w) / ∂w = ∂σ↑(1)w / ∂w = σ↑(1)

综上所述：
    ∂Loss / ∂w↑(1→2) = ∂L(σ) / ∂σ * ∂σ(z) / ∂z * ∂z(w) / ∂w
                     = (σ↑(2) - y) / (σ^2 * (1 - σ↑(2))) * σ↑(2) * (1 - σ↑(2)) * σ↑(1)
                     = σ↑(1) * (σ↑(2) - y)
    ∂Loss / ∂w↑(0→1) = ∂L(σ) / ∂σ↑(2) * ∂σ(z) / ∂z↑(2) * ∂z(σ) / ∂σ↑(1) * ∂σ(z) / ∂z↑(1) * ∂z(w) / ∂w(0→1)
                     = (σ↑(2) - y) * ∂z(σ) / ∂σ↑(1) * ∂σ(z) / ∂z↑(1) * ∂z(w) / ∂w(0→1)
                     = (σ↑(2) - y) * w↑(1→2) * (σ↑(1) * (1 - σ↑(1))) * X
"""
